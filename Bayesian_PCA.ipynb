{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data\n",
      "ingredients =\n",
      "     7    26     6    60\n",
      "     1    29    15    52\n",
      "    11    56     8    20\n",
      "    11    31     8    47\n",
      "     7    52     6    33\n",
      "    11    55     9    22\n",
      "     3    71    17     6\n",
      "     1    31    22    44\n",
      "     2    54    18    22\n",
      "    21    47     4    26\n",
      "     1    40    23    34\n",
      "    11    66     9    12\n",
      "    10    68     8    12\n",
      "Reconstructed data as suggested by MATLAB\n",
      "reconstructedData =\n",
      "    7.0000   26.0000    6.0000   60.0000\n",
      "    1.0000   29.0000   15.0000   52.0000\n",
      "   11.0000   56.0000    8.0000   20.0000\n",
      "   11.0000   31.0000    8.0000   47.0000\n",
      "    7.0000   52.0000    6.0000   33.0000\n",
      "   11.0000   55.0000    9.0000   22.0000\n",
      "    3.0000   71.0000   17.0000    6.0000\n",
      "    1.0000   31.0000   22.0000   44.0000\n",
      "    2.0000   54.0000   18.0000   22.0000\n",
      "   21.0000   47.0000    4.0000   26.0000\n",
      "    1.0000   40.0000   23.0000   34.0000\n",
      "   11.0000   66.0000    9.0000   12.0000\n",
      "   10.0000   68.0000    8.0000   12.0000\n",
      "Reconstructed data via eig(cov)\n",
      "reconstructedData =\n",
      "    7.0000   26.0000    6.0000   60.0000\n",
      "    1.0000   29.0000   15.0000   52.0000\n",
      "   11.0000   56.0000    8.0000   20.0000\n",
      "   11.0000   31.0000    8.0000   47.0000\n",
      "    7.0000   52.0000    6.0000   33.0000\n",
      "   11.0000   55.0000    9.0000   22.0000\n",
      "    3.0000   71.0000   17.0000    6.0000\n",
      "    1.0000   31.0000   22.0000   44.0000\n",
      "    2.0000   54.0000   18.0000   22.0000\n",
      "   21.0000   47.0000    4.0000   26.0000\n",
      "    1.0000   40.0000   23.0000   34.0000\n",
      "   11.0000   66.0000    9.0000   12.0000\n",
      "   10.0000   68.0000    8.0000   12.0000\n"
     ]
    }
   ],
   "source": [
    "load hald\n",
    "\n",
    "[coeff,score,latent,~,~,mu] = pca(ingredients);\n",
    "\n",
    "[dataLen,d] = size(ingredients); % D for number of dimensions\n",
    "reconstructedData = score*coeff'+repmat(mu,dataLen,1);\n",
    "\n",
    "fprintf('Original Data\\n');\n",
    "display(ingredients);\n",
    "\n",
    "fprintf('Reconstructed data as suggested by MATLAB\\n');\n",
    "display(reconstructedData);\n",
    "\n",
    "% pca is just taking the eigenvalue decomposition of the covariance matrix\n",
    "myMu = mean(ingredients,1);\n",
    "tempData = bsxfun(@minus,ingredients,myMu);\n",
    "[V,D] = eig(cov(tempData)); % won't necessarily order the eigenvalues like\n",
    "          % matlab's pca function does\n",
    "% V is now the exact same thing as coeff above (just not in the right order)\n",
    "myScore = tempData/V';\n",
    "reconstructedData = myScore*V'+repmat(myMu,dataLen,1);\n",
    "\n",
    "fprintf('Reconstructed data via eig(cov)\\n');\n",
    "display(reconstructedData);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintaining the full dimensionality in \"score\", you can easily recover the original data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ingredients =\n",
      "     7    26     6    60\n",
      "     1    29    15    52\n",
      "    11    56     8    20\n",
      "    11    31     8    47\n",
      "     7    52     6    33\n",
      "    11    55     9    22\n",
      "     3    71    17     6\n",
      "     1    31    22    44\n",
      "     2    54    18    22\n",
      "    21    47     4    26\n",
      "     1    40    23    34\n",
      "    11    66     9    12\n",
      "    10    68     8    12\n",
      "dimReduceReconstructedData =\n",
      "    6.7992   25.8043    5.7955   59.8079\n",
      "    1.2004   29.1953   15.2041   52.1918\n",
      "   11.5700   56.5555    8.5806   20.5455\n",
      "   11.1916   31.1867    8.1952   47.1834\n",
      "    6.9279   51.9298    5.9266   32.9310\n",
      "   11.0683   55.0666    9.0696   22.0654\n",
      "    2.9586   70.9597   16.9578    5.9604\n",
      "    0.8358   30.8400   21.8328   43.8429\n",
      "    2.2752   54.2682   18.2803   22.2634\n",
      "   20.8276   46.8320    3.8244   25.8351\n",
      "    0.7797   39.7854   22.7756   33.7892\n",
      "   10.7738   65.7796    8.7696   11.7836\n",
      "    9.7917   67.7970    7.7878   11.8006\n",
      "Average absolute error in reconstruction of original data: 0.20\n",
      "Average relative error in reconstruction of original data: 0.026\n"
     ]
    }
   ],
   "source": [
    "reduceCoeff = coeff(:,1:3);\n",
    "reduceScore = score(:,1:3);\n",
    "dimReduceReconstructedData = reduceScore*reduceCoeff'+repmat(mu,dataLen,1);\n",
    "\n",
    "display(ingredients);\n",
    "\n",
    "display(dimReduceReconstructedData);\n",
    "\n",
    "avError = mean(mean(abs(ingredients-dimReduceReconstructedData)));\n",
    "fprintf('Average absolute error in reconstruction of original data: %3.2f\\n',avError);\n",
    "\n",
    "avRelError = mean(mean(abs(ingredients-dimReduceReconstructedData)./ingredients));\n",
    "fprintf('Average relative error in reconstruction of original data: %3.3f\\n',avRelError);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recovered data is almost identical to the original. Thus, the reduceScore matrix is a lower-dimensional representation of the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W provides a full representation of the covariance matrix C\n",
      "Covariance Matrix\n",
      "ans =\n",
      "   34.6026   20.9231  -31.0513  -24.1667\n",
      "   20.9231  242.1410  -13.8782 -253.4167\n",
      "  -31.0513  -13.8782   41.0256    3.1667\n",
      "  -24.1667 -253.4167    3.1667  280.1667\n",
      "Reconstructed covariance matrix from W\n",
      "ans =\n",
      "   34.6026   20.9231  -31.0513  -24.1667\n",
      "   20.9231  242.1410  -13.8782 -253.4167\n",
      "  -31.0513  -13.8782   41.0256    3.1667\n",
      "  -24.1667 -253.4167    3.1667  280.1667\n",
      "Drop the smallest eigenvalue to yield a lower-rank representation\n",
      "ans =\n",
      "     4     3\n",
      "ans =\n",
      "   34.6026   20.9231  -31.0513  -24.1667\n",
      "   20.9231  242.1410  -13.8782 -253.4167\n",
      "  -31.0513  -13.8782   41.0256    3.1667\n",
      "  -24.1667 -253.4167    3.1667  280.1667\n",
      "Drop the first two smallest eigenvalues\n",
      "ans =\n",
      "     4     2\n",
      "ans =\n",
      "   34.2033   24.3198  -30.8564  -21.0581\n",
      "   24.3198  241.8211  -10.9953 -253.5131\n",
      "  -30.8564  -10.9953   41.6520    5.8362\n",
      "  -21.0581 -253.5131    5.8362  280.2595\n"
     ]
    }
   ],
   "source": [
    "[dataLen,d] = size(ingredients); % D for number of dimensions\n",
    "\n",
    "[V,D] = eig(cov(ingredients));\n",
    "sigmasquare = 0;%(1/(d-q))*sum(dropped eigenvalues) and q is number of dropped eigenvalues\n",
    "W = V*sqrtm(D-sigmasquare.*eye(d));\n",
    "\n",
    "fprintf('W provides a full representation of the covariance matrix C\\n');\n",
    "fprintf('Covariance Matrix\\n');\n",
    "cov(ingredients)\n",
    "fprintf('Reconstructed covariance matrix from W\\n');\n",
    "W*W'+sigmasquare.*eye(d)\n",
    "\n",
    "fprintf('Drop the smallest eigenvalue to yield a lower-rank representation\\n');\n",
    "% drop the first and smallest eigenvalue\n",
    "eigenvalues = diag(D);\n",
    "sigmasquare = D(1,1);q = 3;\n",
    "W = V(:,2:end)*sqrtm(D(2:end,2:end)-sigmasquare.*eye(q));\n",
    "size(W)\n",
    "W*W'+sigmasquare.*eye(d)\n",
    "\n",
    "fprintf('Drop the first two smallest eigenvalues\\n');\n",
    "% drop bottom two eigenvalues\n",
    "q = 2;sigmasquare =(1/(d-q))*sum(eigenvalues(1:2));\n",
    "W = V(:,3:end)*sqrtm(D(3:end,3:end)-sigmasquare.*eye(q));\n",
    "size(W)\n",
    "W*W'+sigmasquare.*eye(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping out the first (and smallest) eigenvalue has no effect on the reconstruction \n",
    "of the covariance matrix, despite the fact that the matrix W now has one fewer \n",
    "column. If we drop the first two smallest eigenvalues, then the reconstruction\n",
    "of the covariance matrix causes fairly significant alterations. Thus, probabilistic\n",
    "PCA (PPCA) appears to provide a means by which we can create lower-dimensional\n",
    "representations of the covariance matrix of a dataset. In addition, the original\n",
    "data X can be recovered via the linear transformation X = WZ+sigmasquareI , or \n",
    "X = WZ+mu+sigmasquareI (I is the identity matrix and Z is an unobserved latent\n",
    "variable). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# PPCA (cont.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed Data (Full Transformation Matrix):\n",
      "    7.0000   26.0000    6.0000   60.0000\n",
      "    1.0000   29.0000   15.0000   52.0000\n",
      "   11.0000   56.0000    8.0000   20.0000\n",
      "   11.0000   31.0000    8.0000   47.0000\n",
      "    7.0000   52.0000    6.0000   33.0000\n",
      "   11.0000   55.0000    9.0000   22.0000\n",
      "    3.0000   71.0000   17.0000    6.0000\n",
      "    1.0000   31.0000   22.0000   44.0000\n",
      "    2.0000   54.0000   18.0000   22.0000\n",
      "   21.0000   47.0000    4.0000   26.0000\n",
      "    1.0000   40.0000   23.0000   34.0000\n",
      "   11.0000   66.0000    9.0000   12.0000\n",
      "   10.0000   68.0000    8.0000   12.0000\n",
      "Reconstructed Data (Reduced Transformation Matrix):\n",
      "    6.7992   25.8043    5.7955   59.8079\n",
      "    1.2004   29.1953   15.2041   52.1918\n",
      "   11.5700   56.5555    8.5806   20.5455\n",
      "   11.1916   31.1867    8.1952   47.1834\n",
      "    6.9279   51.9298    5.9266   32.9310\n",
      "   11.0683   55.0666    9.0696   22.0654\n",
      "    2.9586   70.9597   16.9578    5.9604\n",
      "    0.8358   30.8400   21.8328   43.8429\n",
      "    2.2752   54.2682   18.2803   22.2634\n",
      "   20.8276   46.8320    3.8244   25.8351\n",
      "    0.7797   39.7854   22.7756   33.7892\n",
      "   10.7738   65.7796    8.7696   11.7836\n",
      "    9.7917   67.7970    7.7878   11.8006\n",
      "Average absolute error in reconstruction of original data: 0.20\n",
      "Average relative error in reconstruction of original data: 0.026\n"
     ]
    }
   ],
   "source": [
    "[V,D] = eig(cov(ingredients));\n",
    "sigmasquare = 0;%(1/(d-q))*sum(dropped eigenvalues) and q is number of dropped eigenvalues\n",
    "W = V*sqrtm(D-sigmasquare.*eye(d));\n",
    "\n",
    "mu = mean(ingredients,1);\n",
    "\n",
    "% data is now described by the following multivariate normal distribution\n",
    "\n",
    "% N(mu,W*W'+sigmasquare*eye(d));\n",
    "\n",
    "% the model is data = W*latent + mu + noise\n",
    "%   so, data-mu-noise = W*latent\n",
    "%   and inv(W)*(data-mu-noise) = latent\n",
    "\n",
    "latentVars = inv(W)*(ingredients-repmat(mu,dataLen,1))';\n",
    "\n",
    "reconstructedData = W*latentVars+repmat(mu,dataLen,1)';\n",
    "fprintf('Reconstructed Data (Full Transformation Matrix):\\n');\n",
    "display(reconstructedData');\n",
    "\n",
    "eigenvalues = diag(D);\n",
    "sigmasquare = D(1,1);q = 3;\n",
    "W = V(:,2:end)*sqrtm(D(2:end,2:end)-sigmasquare.*eye(q));\n",
    "\n",
    "latentVars = pinv(W)*(ingredients-repmat(mu,dataLen,1))';\n",
    "\n",
    "reconstructedData = W*latentVars+repmat(mu,dataLen,1)';\n",
    "fprintf('Reconstructed Data (Reduced Transformation Matrix):\\n');\n",
    "display(reconstructedData');\n",
    "\n",
    "avError = mean(mean(abs(ingredients-reconstructedData')));\n",
    "fprintf('Average absolute error in reconstruction of original data: %3.2f\\n',avError);\n",
    "\n",
    "avRelError = mean(mean(abs(ingredients-reconstructedData')./ingredients));\n",
    "fprintf('Average relative error in reconstruction of original data: %3.3f\\n',avRelError);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this reconstruction is identical to the reconstruction above using traditional\n",
    "PCA. The errors are also identical. So, PPCA can be used to create a lower-dimensional\n",
    "representation of a dataset (as latent variables), from which the original dataset\n",
    "can be almost exactly reconstructed. You could switch to the latent variable space\n",
    "for analysis, for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The goal of Bayesian PCA is to automatically determine the appropriate number of dimensions to \n",
    "conserve within the set of latent variables (i.e. how many principal components matter?). The original\n",
    "paper from Christopher Bishop states that the \"dimensionality of the latent space has a maximum\n",
    "possible value of q=d-1\". I'm not certain why q=d-1 and not q=d, but I suppose the point is that\n",
    "if q=d, then you haven't really done much in reducing the dimensionality of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100;d = 10;\n",
    "q = d-1;\n",
    "data = zeros(d,N);\n",
    "for ii=1:100\n",
    "    for jj=1:3\n",
    "        data(jj,ii) = normrnd(0,5);\n",
    "    end\n",
    "end\n",
    "for ii=1:100\n",
    "    for jj=4:10\n",
    "        data(jj,ii) = normrnd(0,0.5);\n",
    "    end\n",
    "end\n",
    "\n",
    "load hald;\n",
    "data = ingredients;\n",
    "[N,d]= size(data);\n",
    "data = data';\n",
    "S = cov(data');\n",
    "[V,D] = eig(S);\n",
    "\n",
    "% logLikelihood = -N/2*(d*log(2*pi)+log(det(C))+trace(C\\S));\n",
    "% sigmasquare ~ gamma(a,b) ... from Exact Dimensionality selection for Bayesian PCA\n",
    "% w-(j,k) ~ N(0,1/phi) ... phi is the precision ... b = phi/2\n",
    "% a = sigmasquare/phi\n",
    "%     sigmasquare is the mean of the N-q smallest eigenvalues\n",
    "\n",
    "\n",
    "numIter = 10;\n",
    "q = d-1;\n",
    "\n",
    "fullSize = d*q;\n",
    "params = zeros(fullSize,numIter);\n",
    "posteriorProb = zeros(numIter,1);\n",
    "\n",
    "params(1:fullSize,1) = normrnd(0,1,[fullSize,1]);\n",
    "\n",
    "W = reshape(params(1:fullSize),[d,q]);\n",
    "mu = mean(data,2);\n",
    "sigmasquare = 1;\n",
    "for ii=1:numIter\n",
    "    alpha = zeros(q,1);\n",
    "    for jj=1:q\n",
    "        alpha(jj) = d/norm(W(:,jj)).^2;\n",
    "    end\n",
    "    M = W'*W+sigmasquare*eye(q);\n",
    "    xn = zeros(N,q);\n",
    "    xnxnt = zeros(N,q,q);\n",
    "    for jj=1:N\n",
    "        temp = inv(M)*W'*(data(:,jj)-mu);\n",
    "        xn(jj,:) = temp';\n",
    "        xnxnt(jj,:,:) = sigmasquare*M+temp*temp';\n",
    "    end\n",
    "    \n",
    "    temp = zeros(d,q);temp2 = zeros(q,q);\n",
    "    A = diag(alpha);\n",
    "    for jj=1:N\n",
    "        temp = temp+(data(:,jj)-mu)*xn(jj,:);\n",
    "        temp2 = temp2+squeeze(xnxnt(jj,:,:))+sigmasquare*A;\n",
    "    end\n",
    "    Wprime = temp*inv(temp2);\n",
    "    sigmasquare = 0;\n",
    "    for jj=1:N\n",
    "        sigmasquare = sigmasquare+norm(data(:,jj)-mu).^2-2*xn(jj,:)*Wprime'*(data(:,jj)-mu)+trace(squeeze(xnxnt(jj,:,:))*Wprime'*Wprime);\n",
    "    end\n",
    "    sigmasquare = sigmasquare./(N*d);W = Wprime;\n",
    "end\n",
    "% loglikelihood\n",
    "%  L = -N/2*(d*ln(2*pi)+ln(det(C))+trace(inv(C)*S));\n",
    "% log posterior = L - 0.5*sum{i=1 to d-1} [alpha-i*norm(w-i).^2]+const\n",
    "% loglikelihood\n",
    "%  L = -N/2*(d*ln(2*pi)+ln(det(C))+trace(inv(C)*S));\n",
    "% log posterior = L - 0.5*sum{i=1 to d-1} [alpha-i*norm(w-i).^2]+const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Matlab",
   "language": "matlab",
   "name": "matlab"
  },
  "language_info": {
   "codemirror_mode": "octave",
   "file_extension": ".m",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-matlab",
   "name": "matlab",
   "version": "0.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
